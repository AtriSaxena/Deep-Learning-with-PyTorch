{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back-propagation algorithm \n",
    "\n",
    "PyTorch provides a very simple automatic derivation to help us solve the derivative. For simpler models, we can also manually determine the gradient of the parameters, but for very complex models, such as a 100-layer network, how can we effectively manually Find this gradient? Here we need to introduce a back propagation algorithm. The essence of automatic derivation is a back propagation algorithm.\n",
    "\n",
    "The backpropagation algorithm is an algorithm for effectively solving the gradient. It is essentially the application of a chained derivation rule. However, this simple and obvious method was invented nearly 30 years after Roseblatt proposed the perceptron algorithm. Popular, Bengio said: \"A lot of seemingly obvious ideas become apparent only afterwards.\"\n",
    "\n",
    "Let's take a closer look at what is a backpropagation algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain rule \n",
    "\n",
    "Suppose that we have two functions \n",
    "$$f\n",
    "(\n",
    "x\n",
    ")$$\n",
    " and \n",
    "$$g\n",
    "(\n",
    "x\n",
    ")$$\n",
    " and they are both differentiable.\n",
    "\n",
    "1. If we define \n",
    "F\n",
    "(\n",
    "x\n",
    ")\n",
    "=\n",
    "(\n",
    "f\n",
    "∘\n",
    "g\n",
    ")\n",
    "(\n",
    "x\n",
    ")\n",
    " then the derivative of \n",
    "F\n",
    "(\n",
    "x\n",
    ")\n",
    " is,\n",
    "$$F\n",
    "′\n",
    "(\n",
    "x\n",
    ")\n",
    "=\n",
    "f\n",
    "′\n",
    "(\n",
    "g\n",
    "(\n",
    "x\n",
    ")\n",
    ")\n",
    "g\n",
    "′\n",
    "(\n",
    "x\n",
    ")$$\n",
    "\n",
    "2. If we have \n",
    "y\n",
    "=\n",
    "f\n",
    "(\n",
    "u\n",
    ")\n",
    " and \n",
    "u\n",
    "=\n",
    "g\n",
    "(\n",
    "x\n",
    ")\n",
    " then the derivative of \n",
    "y\n",
    " is,\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x} = \\frac{\\partial y}{\\partial u}\\frac{\\partial u}{\\partial x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation Algorithm\n",
    "\n",
    "Understand the chain rule, we can start to introduce the back propagation algorithm. In essence, the back propagation algorithm is only an application of the chain rule. We will use the example ```q=x + y``` , ```f= qz``` from This calculation process can be expressed by calculating the graph.\n",
    "![](https://ws1.sinaimg.cn/large/006tNc79ly1fmiozcinyzj30c806vglk.jpg)\n",
    "\n",
    "\n",
    "The green number above indicates its value, and the red number below indicates the gradient obtained. We can look at the implementation of the backpropagation algorithm step by step. First from the end, the gradient is of course 1, then calculated\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial q} = z = -4,\\ \\frac{\\partial f}{\\partial z} = q = 3$$\n",
    "Then we calculate\n",
    "$$\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial q} \\frac{\\partial q}{\\partial x} = -4 \\times 1 = -4,\\ \\frac{\\partial f}{\\partial y} = \\frac{\\partial f}{\\partial q} \\frac{\\partial q}{\\partial y} = -4 \\times 1 = -4$$\n",
    "\n",
    "So step by step we have found $ \\nabla f(x,y,z) $"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
