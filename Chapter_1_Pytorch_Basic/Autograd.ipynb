{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic derivation of simple \n",
    "Below we show some simple cases of automatic derivation, \"simple\" is reflected in the calculation results are scalar, that is, a number, we automatically deduct this scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([19.], grad_fn=<AddBackward>)\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.Tensor([2]), requires_grad=True)\n",
    "y = x + 2\n",
    "z = y ** 2 + 3\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the above column operations, we get the final result out from x, we can represent it as a mathematical formula\n",
    "\n",
    "$$\n",
    "z = (x + 2)^2 + 3\n",
    "$$\n",
    " \n",
    "Then the result of our derivation from z to x is\n",
    "$$\n",
    "∂ z∂ x= 2 ( x+ 2 ) = 2 ( 2 + 2 ) = 8\n",
    "∂z∂x=2(x+2)=2(2+2)=8\n",
    " $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8.])\n"
     ]
    }
   ],
   "source": [
    "#using automatic derviation\n",
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a simple example like this, we verified the automatic derivation and found that it is very convenient to use automatic derivation. If it's a more complicated example, then manual derivation can be very troublesome, so the auto-derivation mechanism can help us save the troublesome mathematics. Let's look at a more complicated example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.randn(10, 20), requires_grad=True)\n",
    "y = Variable(torch.randn(10, 5), requires_grad=True)\n",
    "w = Variable(torch.randn(20, 5), requires_grad=True)\n",
    "\n",
    "out = torch.mean(y - torch.matmul(x, w)) # torch.matmul doing matrix multiplication\n",
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0423,  0.0249,  0.0351, -0.0131,  0.0017,  0.0439, -0.0653,  0.0279,\n",
      "          0.0104,  0.0057,  0.0304,  0.0337, -0.0452, -0.0746, -0.0282, -0.0258,\n",
      "         -0.0400,  0.0103,  0.0762,  0.1168],\n",
      "        [-0.0423,  0.0249,  0.0351, -0.0131,  0.0017,  0.0439, -0.0653,  0.0279,\n",
      "          0.0104,  0.0057,  0.0304,  0.0337, -0.0452, -0.0746, -0.0282, -0.0258,\n",
      "         -0.0400,  0.0103,  0.0762,  0.1168],\n",
      "        [-0.0423,  0.0249,  0.0351, -0.0131,  0.0017,  0.0439, -0.0653,  0.0279,\n",
      "          0.0104,  0.0057,  0.0304,  0.0337, -0.0452, -0.0746, -0.0282, -0.0258,\n",
      "         -0.0400,  0.0103,  0.0762,  0.1168],\n",
      "        [-0.0423,  0.0249,  0.0351, -0.0131,  0.0017,  0.0439, -0.0653,  0.0279,\n",
      "          0.0104,  0.0057,  0.0304,  0.0337, -0.0452, -0.0746, -0.0282, -0.0258,\n",
      "         -0.0400,  0.0103,  0.0762,  0.1168],\n",
      "        [-0.0423,  0.0249,  0.0351, -0.0131,  0.0017,  0.0439, -0.0653,  0.0279,\n",
      "          0.0104,  0.0057,  0.0304,  0.0337, -0.0452, -0.0746, -0.0282, -0.0258,\n",
      "         -0.0400,  0.0103,  0.0762,  0.1168],\n",
      "        [-0.0423,  0.0249,  0.0351, -0.0131,  0.0017,  0.0439, -0.0653,  0.0279,\n",
      "          0.0104,  0.0057,  0.0304,  0.0337, -0.0452, -0.0746, -0.0282, -0.0258,\n",
      "         -0.0400,  0.0103,  0.0762,  0.1168],\n",
      "        [-0.0423,  0.0249,  0.0351, -0.0131,  0.0017,  0.0439, -0.0653,  0.0279,\n",
      "          0.0104,  0.0057,  0.0304,  0.0337, -0.0452, -0.0746, -0.0282, -0.0258,\n",
      "         -0.0400,  0.0103,  0.0762,  0.1168],\n",
      "        [-0.0423,  0.0249,  0.0351, -0.0131,  0.0017,  0.0439, -0.0653,  0.0279,\n",
      "          0.0104,  0.0057,  0.0304,  0.0337, -0.0452, -0.0746, -0.0282, -0.0258,\n",
      "         -0.0400,  0.0103,  0.0762,  0.1168],\n",
      "        [-0.0423,  0.0249,  0.0351, -0.0131,  0.0017,  0.0439, -0.0653,  0.0279,\n",
      "          0.0104,  0.0057,  0.0304,  0.0337, -0.0452, -0.0746, -0.0282, -0.0258,\n",
      "         -0.0400,  0.0103,  0.0762,  0.1168],\n",
      "        [-0.0423,  0.0249,  0.0351, -0.0131,  0.0017,  0.0439, -0.0653,  0.0279,\n",
      "          0.0104,  0.0057,  0.0304,  0.0337, -0.0452, -0.0746, -0.0282, -0.0258,\n",
      "         -0.0400,  0.0103,  0.0762,  0.1168]])\n"
     ]
    }
   ],
   "source": [
    "# get the gradient of x\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200]])\n"
     ]
    }
   ],
   "source": [
    "#get the gradient of y\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0205, -0.0205, -0.0205, -0.0205, -0.0205],\n",
      "        [-0.0415, -0.0415, -0.0415, -0.0415, -0.0415],\n",
      "        [ 0.0406,  0.0406,  0.0406,  0.0406,  0.0406],\n",
      "        [ 0.0505,  0.0505,  0.0505,  0.0505,  0.0505],\n",
      "        [-0.0064, -0.0064, -0.0064, -0.0064, -0.0064],\n",
      "        [ 0.0154,  0.0154,  0.0154,  0.0154,  0.0154],\n",
      "        [ 0.1067,  0.1067,  0.1067,  0.1067,  0.1067],\n",
      "        [-0.0853, -0.0853, -0.0853, -0.0853, -0.0853],\n",
      "        [-0.1139, -0.1139, -0.1139, -0.1139, -0.1139],\n",
      "        [ 0.0826,  0.0826,  0.0826,  0.0826,  0.0826],\n",
      "        [-0.0833, -0.0833, -0.0833, -0.0833, -0.0833],\n",
      "        [ 0.0150,  0.0150,  0.0150,  0.0150,  0.0150],\n",
      "        [-0.0652, -0.0652, -0.0652, -0.0652, -0.0652],\n",
      "        [ 0.0135,  0.0135,  0.0135,  0.0135,  0.0135],\n",
      "        [-0.0435, -0.0435, -0.0435, -0.0435, -0.0435],\n",
      "        [ 0.1030,  0.1030,  0.1030,  0.1030,  0.1030],\n",
      "        [ 0.0537,  0.0537,  0.0537,  0.0537,  0.0537],\n",
      "        [-0.0519, -0.0519, -0.0519, -0.0519, -0.0519],\n",
      "        [-0.1028, -0.1028, -0.1028, -0.1028, -0.1028],\n",
      "        [ 0.0120,  0.0120,  0.0120,  0.0120,  0.0120]])\n"
     ]
    }
   ],
   "source": [
    "#get the gradient of w\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above mathematical formula is more complicated. After matrix multiplication, the corresponding elements of the two matrices are multiplied, and then all the elements are averaged. Interested students can manually calculate the gradient. Using PyTorch's automatic derivation, we can easily get x. The derivatives of y and w, because deep learning is full of a large number of matrix operations, so we have no way to manually find these derivatives, with automatic derivation can easily solve the problem of network updates."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
