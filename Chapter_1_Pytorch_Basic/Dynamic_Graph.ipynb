{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic & Static\n",
    "\n",
    "At present, the neural network framework is divided into a static graph framework and a dynamic graph framework. The biggest difference between PyTorch and TensorFlow, Caffe and other frameworks is that they have different computational graph representations. TensorFlow uses static graphs, which means that we first define the computation graph and then use it continuously, and in PyTorch, we rebuild a new computation graph each time. Through this course, we will understand the advantages and disadvantages between static and dynamic images.\n",
    "\n",
    "For the user, there are very big differences between the two forms of calculation graphs. At the same time, static graphs and dynamic graphs have their own advantages. For example, dynamic graphs are more convenient for debugging, and users can debug in any way they like. At the same time, it is very intuitive, and the static graph is defined by running it first. After running it again, it is no longer necessary to rebuild the graph, so the speed will be faster than the dynamic graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://ws3.sinaimg.cn/large/006tNc79ly1fmai482qumg30rs0fmq6e.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow: Static Graph\n",
    "\n",
    "In TensorFlow, we define the computational graph once and then execute the same graph over and over again, possibly feeding different input data to the graph. \n",
    "Here we use TensorFlow to fit a simple two-layer net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Codeseeder\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Code in file autograd/tf_two_layer_net.py\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# First we set up the computational graph:\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create placeholders for the input and target data; these will be filled\n",
    "# with real data when we execute the graph.\n",
    "x = tf.placeholder(tf.float32, shape=(None, D_in))\n",
    "y = tf.placeholder(tf.float32, shape=(None, D_out))\n",
    "\n",
    "# Create Variables for the weights and initialize them with random data.\n",
    "# A TensorFlow Variable persists its value across executions of the graph.\n",
    "w1 = tf.Variable(tf.random_normal((D_in, H)))\n",
    "w2 = tf.Variable(tf.random_normal((H, D_out)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass: Compute the predicted y using operations on TensorFlow Tensors.\n",
    "# Note that this code does not actually perform any numeric operations; it\n",
    "# merely sets up the computational graph that we will later execute.\n",
    "h = tf.matmul(x, w1)\n",
    "h_relu = tf.maximum(h, tf.zeros(1))\n",
    "y_pred = tf.matmul(h_relu, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute loss using operations on TensorFlow Tensors\n",
    "loss = tf.reduce_sum((y - y_pred) ** 2.0)\n",
    "\n",
    "# Compute gradient of the loss with respect to w1 and w2.\n",
    "grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])\n",
    "\n",
    "# Update the weights using gradient descent. To actually update the weights\n",
    "# we need to evaluate new_w1 and new_w2 when executing the graph. Note that\n",
    "# in TensorFlow the the act of updating the value of the weights is part of\n",
    "# the computational graph; in PyTorch this happens outside the computational\n",
    "# graph.\n",
    "learning_rate = 1e-6\n",
    "new_w1 = w1.assign(w1 - learning_rate * grad_w1)\n",
    "new_w2 = w2.assign(w2 - learning_rate * grad_w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27855396.0\n",
      "22304114.0\n",
      "20955164.0\n",
      "20435418.0\n",
      "19126156.0\n",
      "16154638.0\n",
      "12239609.0\n",
      "8337312.5\n",
      "5337310.0\n",
      "3323413.0\n",
      "2100637.5\n",
      "1379123.2\n",
      "955671.5\n",
      "699161.5\n",
      "536945.2\n",
      "428586.94\n",
      "352145.7\n",
      "295298.16\n",
      "251192.06\n",
      "215878.75\n",
      "186988.39\n",
      "162916.22\n",
      "142595.66\n",
      "125296.69\n",
      "110459.336\n",
      "97668.375\n",
      "86587.92\n",
      "76945.73\n",
      "68529.28\n",
      "61161.36\n",
      "54693.164\n",
      "48998.867\n",
      "43974.984\n",
      "39530.066\n",
      "35594.78\n",
      "32099.953\n",
      "28991.652\n",
      "26220.05\n",
      "23745.195\n",
      "21529.629\n",
      "19543.95\n",
      "17761.402\n",
      "16157.533\n",
      "14714.482\n",
      "13412.182\n",
      "12235.715\n",
      "11172.024\n",
      "10209.08\n",
      "9336.236\n",
      "8544.175\n",
      "7825.163\n",
      "7171.174\n",
      "6576.0796\n",
      "6034.2227\n",
      "5540.472\n",
      "5089.7773\n",
      "4678.326\n",
      "4302.56\n",
      "3959.326\n",
      "3644.7527\n",
      "3356.7056\n",
      "3092.2358\n",
      "2849.9905\n",
      "2627.6777\n",
      "2423.7437\n",
      "2236.565\n",
      "2064.6602\n",
      "1906.6196\n",
      "1761.2465\n",
      "1627.5468\n",
      "1504.5259\n",
      "1391.2698\n",
      "1286.8767\n",
      "1190.6262\n",
      "1101.9138\n",
      "1020.16144\n",
      "944.67206\n",
      "875.01965\n",
      "810.72144\n",
      "751.32556\n",
      "696.5172\n",
      "645.80994\n",
      "598.93677\n",
      "555.5604\n",
      "515.43\n",
      "478.34186\n",
      "443.97736\n",
      "412.1577\n",
      "382.7119\n",
      "355.43677\n",
      "330.17737\n",
      "306.7508\n",
      "285.0323\n",
      "264.89874\n",
      "246.23602\n",
      "228.92667\n",
      "212.86612\n",
      "197.96039\n",
      "184.12537\n",
      "171.28299\n",
      "159.36926\n",
      "148.31435\n",
      "138.03198\n",
      "128.48795\n",
      "119.62464\n",
      "111.390366\n",
      "103.73593\n",
      "96.62274\n",
      "90.00487\n",
      "83.84933\n",
      "78.124626\n",
      "72.79979\n",
      "67.84694\n",
      "63.239582\n",
      "58.948933\n",
      "54.957943\n",
      "51.240982\n",
      "47.78318\n",
      "44.563866\n",
      "41.564873\n",
      "38.77099\n",
      "36.167637\n",
      "33.74245\n",
      "31.48267\n",
      "29.378035\n",
      "27.418901\n",
      "25.590973\n",
      "23.886578\n",
      "22.297535\n",
      "20.815865\n",
      "19.435612\n",
      "18.148256\n",
      "16.948257\n",
      "15.828069\n",
      "14.7829685\n",
      "13.808554\n",
      "12.898403\n",
      "12.049549\n",
      "11.258263\n",
      "10.520014\n",
      "9.829025\n",
      "9.185863\n",
      "8.584963\n",
      "8.023159\n",
      "7.499662\n",
      "7.010456\n",
      "6.5535107\n",
      "6.1268363\n",
      "5.728792\n",
      "5.356467\n",
      "5.0088005\n",
      "4.6838446\n",
      "4.380596\n",
      "4.097038\n",
      "3.8320174\n",
      "3.5845485\n",
      "3.353283\n",
      "3.136949\n",
      "2.934498\n",
      "2.7455344\n",
      "2.569025\n",
      "2.4038985\n",
      "2.2494237\n",
      "2.105215\n",
      "1.9700435\n",
      "1.843853\n",
      "1.7258232\n",
      "1.615559\n",
      "1.5123119\n",
      "1.4157307\n",
      "1.3253899\n",
      "1.240896\n",
      "1.1618178\n",
      "1.0877724\n",
      "1.0185643\n",
      "0.95384777\n",
      "0.89323\n",
      "0.8364228\n",
      "0.7836053\n",
      "0.73382837\n",
      "0.6873068\n",
      "0.6438083\n",
      "0.603099\n",
      "0.5649229\n",
      "0.52916163\n",
      "0.4957217\n",
      "0.46447414\n",
      "0.435243\n",
      "0.407836\n",
      "0.38205916\n",
      "0.35804647\n",
      "0.33551222\n",
      "0.3143985\n",
      "0.2946184\n",
      "0.27615315\n",
      "0.25879472\n",
      "0.24254617\n",
      "0.22735356\n",
      "0.21309969\n",
      "0.1998056\n",
      "0.18730144\n",
      "0.17558023\n",
      "0.16461654\n",
      "0.1542992\n",
      "0.14465702\n",
      "0.13568352\n",
      "0.12722835\n",
      "0.11928055\n",
      "0.1118562\n",
      "0.10488926\n",
      "0.098361924\n",
      "0.09220877\n",
      "0.086493015\n",
      "0.08109743\n",
      "0.07608068\n",
      "0.071327284\n",
      "0.0669335\n",
      "0.062766306\n",
      "0.058869764\n",
      "0.055229597\n",
      "0.05182024\n",
      "0.04860783\n",
      "0.045605242\n",
      "0.042788245\n",
      "0.040156413\n",
      "0.03766655\n",
      "0.035318077\n",
      "0.033155415\n",
      "0.031116743\n",
      "0.029190656\n",
      "0.027387662\n",
      "0.025717992\n",
      "0.024134412\n",
      "0.022654017\n",
      "0.021268578\n",
      "0.01997091\n",
      "0.018747298\n",
      "0.017601313\n",
      "0.016532047\n",
      "0.015518856\n",
      "0.014565093\n",
      "0.013676362\n",
      "0.012851818\n",
      "0.012063013\n",
      "0.011334166\n",
      "0.010642955\n",
      "0.010008769\n",
      "0.009400404\n",
      "0.008831822\n",
      "0.008308539\n",
      "0.007805729\n",
      "0.007334858\n",
      "0.0069032907\n",
      "0.0064914557\n",
      "0.006107161\n",
      "0.005749926\n",
      "0.005410391\n",
      "0.0051006293\n",
      "0.00479542\n",
      "0.0045124195\n",
      "0.0042520585\n",
      "0.0040017883\n",
      "0.0037706706\n",
      "0.0035571114\n",
      "0.0033571515\n",
      "0.0031659435\n",
      "0.002989655\n",
      "0.0028235426\n",
      "0.0026683232\n",
      "0.002521943\n",
      "0.0023823753\n",
      "0.0022483137\n",
      "0.002129349\n",
      "0.0020158757\n",
      "0.001907492\n",
      "0.0018079294\n",
      "0.001711371\n",
      "0.0016260381\n",
      "0.0015414096\n",
      "0.0014612584\n",
      "0.0013851863\n",
      "0.0013180971\n",
      "0.0012549188\n",
      "0.0011927837\n",
      "0.0011301669\n",
      "0.0010776223\n",
      "0.0010240523\n",
      "0.0009753084\n",
      "0.00092849415\n",
      "0.00088583195\n",
      "0.00084537495\n",
      "0.00080545223\n",
      "0.0007692808\n",
      "0.00073356193\n",
      "0.0007030166\n",
      "0.00067334686\n",
      "0.00064304436\n",
      "0.00061514403\n",
      "0.0005903514\n",
      "0.00056533806\n",
      "0.00054251193\n",
      "0.0005190582\n",
      "0.0004986926\n",
      "0.0004794885\n",
      "0.00045813306\n",
      "0.0004390576\n",
      "0.0004228548\n",
      "0.00040609375\n",
      "0.00039042282\n",
      "0.0003760191\n",
      "0.00036056925\n",
      "0.0003473162\n",
      "0.0003342707\n",
      "0.00032284032\n",
      "0.0003117536\n",
      "0.00030000234\n",
      "0.0002893746\n",
      "0.0002801226\n",
      "0.0002705775\n",
      "0.00026135528\n",
      "0.0002525682\n",
      "0.000244102\n",
      "0.00023584625\n",
      "0.00022835474\n",
      "0.0002219362\n",
      "0.00021447256\n",
      "0.0002075461\n",
      "0.00020113564\n",
      "0.00019477916\n",
      "0.00018909835\n",
      "0.00018216425\n",
      "0.0001766738\n",
      "0.0001718119\n",
      "0.00016759298\n",
      "0.00016248091\n",
      "0.00015810551\n",
      "0.0001527966\n",
      "0.00014863987\n",
      "0.00014481925\n",
      "0.00014037134\n",
      "0.00013684331\n",
      "0.00013274394\n",
      "0.0001292418\n",
      "0.00012596496\n",
      "0.00012306566\n",
      "0.00011956239\n",
      "0.000116319556\n",
      "0.00011386975\n",
      "0.000110918\n",
      "0.00010871554\n",
      "0.00010608927\n",
      "0.00010309354\n",
      "0.00010006346\n",
      "9.813938e-05\n",
      "9.561388e-05\n",
      "9.303428e-05\n",
      "9.129653e-05\n",
      "8.928591e-05\n",
      "8.696778e-05\n",
      "8.5478285e-05\n",
      "8.350407e-05\n",
      "8.2083585e-05\n",
      "8.02549e-05\n",
      "7.8296194e-05\n",
      "7.65844e-05\n",
      "7.506979e-05\n",
      "7.3668045e-05\n",
      "7.241922e-05\n",
      "7.0634254e-05\n",
      "6.9157635e-05\n",
      "6.7612e-05\n",
      "6.632942e-05\n",
      "6.512068e-05\n",
      "6.3799685e-05\n",
      "6.305011e-05\n",
      "6.181115e-05\n",
      "6.0538365e-05\n",
      "5.9347716e-05\n",
      "5.80629e-05\n",
      "5.6933703e-05\n",
      "5.5993307e-05\n",
      "5.5127166e-05\n",
      "5.44653e-05\n",
      "5.3237312e-05\n",
      "5.258844e-05\n",
      "5.1790994e-05\n",
      "5.070515e-05\n",
      "4.9616057e-05\n",
      "4.864236e-05\n",
      "4.814087e-05\n",
      "4.7278492e-05\n",
      "4.643046e-05\n",
      "4.5583278e-05\n",
      "4.5073462e-05\n",
      "4.4601806e-05\n",
      "4.3643286e-05\n",
      "4.298349e-05\n",
      "4.1929306e-05\n",
      "4.13213e-05\n",
      "4.0568e-05\n",
      "4.0051178e-05\n",
      "3.9422033e-05\n",
      "3.9273207e-05\n",
      "3.82516e-05\n",
      "3.7580856e-05\n",
      "3.6916936e-05\n",
      "3.6644364e-05\n",
      "3.6223882e-05\n",
      "3.5665278e-05\n",
      "3.527246e-05\n",
      "3.473884e-05\n",
      "3.4314577e-05\n",
      "3.3806693e-05\n",
      "3.3637272e-05\n",
      "3.293623e-05\n",
      "3.2704946e-05\n",
      "3.2277203e-05\n",
      "3.190143e-05\n",
      "3.148366e-05\n",
      "3.0872918e-05\n",
      "3.0485453e-05\n",
      "3.0041872e-05\n",
      "2.9713494e-05\n",
      "2.9298228e-05\n",
      "2.8799306e-05\n",
      "2.8425944e-05\n",
      "2.7937298e-05\n",
      "2.7737715e-05\n",
      "2.7259368e-05\n",
      "2.685195e-05\n",
      "2.6684913e-05\n",
      "2.6276863e-05\n",
      "2.5839134e-05\n",
      "2.5600968e-05\n",
      "2.552239e-05\n",
      "2.5208603e-05\n",
      "2.4978237e-05\n",
      "2.462477e-05\n",
      "2.4311006e-05\n",
      "2.407163e-05\n",
      "2.3692399e-05\n",
      "2.3548015e-05\n",
      "2.3234112e-05\n",
      "2.3027907e-05\n",
      "2.272431e-05\n",
      "2.2408472e-05\n",
      "2.215453e-05\n",
      "2.1997022e-05\n",
      "2.19581e-05\n",
      "2.1507865e-05\n",
      "2.1288712e-05\n",
      "2.1179154e-05\n",
      "2.1055515e-05\n",
      "2.0783895e-05\n",
      "2.0336833e-05\n",
      "2.0277155e-05\n",
      "2.0149586e-05\n",
      "1.9823394e-05\n",
      "1.9702991e-05\n",
      "1.9413947e-05\n",
      "1.9245204e-05\n",
      "1.896562e-05\n",
      "1.8806904e-05\n",
      "1.8723862e-05\n",
      "1.8547598e-05\n",
      "1.8279075e-05\n",
      "1.8107414e-05\n",
      "1.8010578e-05\n",
      "1.7727969e-05\n",
      "1.7532328e-05\n",
      "1.7397439e-05\n",
      "1.7265951e-05\n",
      "1.7129567e-05\n",
      "1.6926553e-05\n",
      "1.673475e-05\n",
      "1.6622489e-05\n",
      "1.6611244e-05\n",
      "1.6478167e-05\n",
      "1.6223179e-05\n",
      "1.6126429e-05\n",
      "1.6005422e-05\n",
      "1.5803756e-05\n",
      "1.553744e-05\n",
      "1.5333633e-05\n",
      "1.5348909e-05\n",
      "1.53435e-05\n",
      "1.5227843e-05\n",
      "1.5027294e-05\n",
      "1.4852425e-05\n",
      "1.4762387e-05\n",
      "1.4635046e-05\n",
      "1.44873975e-05\n",
      "1.4377443e-05\n",
      "1.4216572e-05\n",
      "1.4121035e-05\n",
      "1.390754e-05\n",
      "1.3939913e-05\n",
      "1.386193e-05\n",
      "1.3855675e-05\n",
      "1.3695876e-05\n"
     ]
    }
   ],
   "source": [
    "# Now we have built our computational graph, so we enter a TensorFlow session to\n",
    "# actually execute the graph.\n",
    "with tf.Session() as sess:\n",
    "  # Run the graph once to initialize the Variables w1 and w2.\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "\n",
    "  # Create numpy arrays holding the actual data for the inputs x and targets y\n",
    "  x_value = np.random.randn(N, D_in)\n",
    "  y_value = np.random.randn(N, D_out)\n",
    "  for _ in range(500):\n",
    "    # Execute the graph many times. Each time it executes we want to bind\n",
    "    # x_value to x and y_value to y, specified with the feed_dict argument.\n",
    "    # Each time we execute the graph we want to compute the values for loss,\n",
    "    # new_w1, and new_w2; the values of these Tensors are returned as numpy\n",
    "    # arrays.\n",
    "    loss_value, _, _ = sess.run([loss, new_w1, new_w2],\n",
    "                                feed_dict={x: x_value, y: y_value})\n",
    "    print(loss_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch: Dynamic Graph\n",
    "\n",
    "Here we use PyTorch Tensors and autograd to implement our two-layer network.\n",
    "\n",
    "When using autograd, the forward pass of your network will define a computational graph; nodes in the graph will be Tensors, and edges will be functions that produce output Tensors from input Tensors. Backpropagating through this graph then allows you to easily compute gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda') # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random Tensors to hold input and outputs\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# Create random Tensors for weights; setting requires_grad=True means that we\n",
    "# want to compute gradients for these Tensors during the backward pass.\n",
    "w1 = torch.randn(D_in, H, device=device, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 34676156.0\n",
      "1 31003460.0\n",
      "2 28006888.0\n",
      "3 22690798.0\n",
      "4 16010568.0\n",
      "5 9993319.0\n",
      "6 5935406.0\n",
      "7 3604461.0\n",
      "8 2358743.25\n",
      "9 1681317.5\n",
      "10 1287884.375\n",
      "11 1036888.6875\n",
      "12 861576.6875\n",
      "13 730156.125\n",
      "14 627009.0625\n",
      "15 543271.5\n",
      "16 473835.0\n",
      "17 415600.15625\n",
      "18 366345.21875\n",
      "19 324227.21875\n",
      "20 287952.15625\n",
      "21 256592.421875\n",
      "22 229325.734375\n",
      "23 205518.171875\n",
      "24 184644.296875\n",
      "25 166278.171875\n",
      "26 150046.5\n",
      "27 135670.0\n",
      "28 122907.6875\n",
      "29 111545.2109375\n",
      "30 101396.140625\n",
      "31 92318.265625\n",
      "32 84202.4609375\n",
      "33 76903.265625\n",
      "34 70328.328125\n",
      "35 64399.71875\n",
      "36 59044.68359375\n",
      "37 54194.0625\n",
      "38 49795.609375\n",
      "39 45799.80078125\n",
      "40 42170.5390625\n",
      "41 38869.48046875\n",
      "42 35859.8203125\n",
      "43 33114.39453125\n",
      "44 30604.767578125\n",
      "45 28307.232421875\n",
      "46 26203.96875\n",
      "47 24276.0390625\n",
      "48 22505.15234375\n",
      "49 20877.240234375\n",
      "50 19379.43359375\n",
      "51 18000.31640625\n",
      "52 16728.583984375\n",
      "53 15556.255859375\n",
      "54 14474.2626953125\n",
      "55 13475.0244140625\n",
      "56 12550.2607421875\n",
      "57 11695.1162109375\n",
      "58 10903.232421875\n",
      "59 10170.10546875\n",
      "60 9490.240234375\n",
      "61 8859.5771484375\n",
      "62 8274.4580078125\n",
      "63 7730.9658203125\n",
      "64 7225.98046875\n",
      "65 6756.97314453125\n",
      "66 6320.71435546875\n",
      "67 5914.4638671875\n",
      "68 5536.2724609375\n",
      "69 5183.994140625\n",
      "70 4855.81201171875\n",
      "71 4549.62841796875\n",
      "72 4264.001953125\n",
      "73 3997.747802734375\n",
      "74 3749.000732421875\n",
      "75 3516.952392578125\n",
      "76 3300.31884765625\n",
      "77 3097.748046875\n",
      "78 2908.33984375\n",
      "79 2731.300537109375\n",
      "80 2565.647705078125\n",
      "81 2410.59375\n",
      "82 2265.416259765625\n",
      "83 2129.5654296875\n",
      "84 2002.276611328125\n",
      "85 1882.9915771484375\n",
      "86 1771.16650390625\n",
      "87 1666.35595703125\n",
      "88 1568.0247802734375\n",
      "89 1475.755126953125\n",
      "90 1389.2227783203125\n",
      "91 1308.00390625\n",
      "92 1231.718505859375\n",
      "93 1160.0869140625\n",
      "94 1092.83056640625\n",
      "95 1029.6807861328125\n",
      "96 970.3009643554688\n",
      "97 914.4690551757812\n",
      "98 862.0059204101562\n",
      "99 812.6815185546875\n",
      "100 766.2767944335938\n",
      "101 722.65234375\n",
      "102 681.596435546875\n",
      "103 642.9784545898438\n",
      "104 606.6371459960938\n",
      "105 572.4155883789062\n",
      "106 540.1865844726562\n",
      "107 509.8442077636719\n",
      "108 481.2746887207031\n",
      "109 454.3511657714844\n",
      "110 428.9990234375\n",
      "111 405.08941650390625\n",
      "112 382.5640869140625\n",
      "113 361.3405456542969\n",
      "114 341.33782958984375\n",
      "115 322.4684753417969\n",
      "116 304.67462158203125\n",
      "117 287.8911437988281\n",
      "118 272.0601806640625\n",
      "119 257.1266784667969\n",
      "120 243.03736877441406\n",
      "121 229.7444305419922\n",
      "122 217.19818115234375\n",
      "123 205.3541259765625\n",
      "124 194.17819213867188\n",
      "125 183.62301635742188\n",
      "126 173.65818786621094\n",
      "127 164.2488555908203\n",
      "128 155.36276245117188\n",
      "129 146.96401977539062\n",
      "130 139.03651428222656\n",
      "131 131.5449981689453\n",
      "132 124.47332763671875\n",
      "133 117.7873764038086\n",
      "134 111.46913146972656\n",
      "135 105.49830627441406\n",
      "136 99.85714721679688\n",
      "137 94.52027893066406\n",
      "138 89.4772720336914\n",
      "139 84.71179962158203\n",
      "140 80.20352172851562\n",
      "141 75.94031524658203\n",
      "142 71.90852355957031\n",
      "143 68.09575653076172\n",
      "144 64.48858642578125\n",
      "145 61.07666778564453\n",
      "146 57.8505859375\n",
      "147 54.79706573486328\n",
      "148 51.90731430053711\n",
      "149 49.17353057861328\n",
      "150 46.58683395385742\n",
      "151 44.13817596435547\n",
      "152 41.82041931152344\n",
      "153 39.626766204833984\n",
      "154 37.551116943359375\n",
      "155 35.586029052734375\n",
      "156 33.72581481933594\n",
      "157 31.96411895751953\n",
      "158 30.296966552734375\n",
      "159 28.71701431274414\n",
      "160 27.22088050842285\n",
      "161 25.80415153503418\n",
      "162 24.4631290435791\n",
      "163 23.1922550201416\n",
      "164 21.98830223083496\n",
      "165 20.84818458557129\n",
      "166 19.768762588500977\n",
      "167 18.745025634765625\n",
      "168 17.775466918945312\n",
      "169 16.857358932495117\n",
      "170 15.988941192626953\n",
      "171 15.166215896606445\n",
      "172 14.387022972106934\n",
      "173 13.647432327270508\n",
      "174 12.947494506835938\n",
      "175 12.283769607543945\n",
      "176 11.654414176940918\n",
      "177 11.057720184326172\n",
      "178 10.49299430847168\n",
      "179 9.9561128616333\n",
      "180 9.447671890258789\n",
      "181 8.966094017028809\n",
      "182 8.508330345153809\n",
      "183 8.074953079223633\n",
      "184 7.6641130447387695\n",
      "185 7.274158954620361\n",
      "186 6.903904914855957\n",
      "187 6.553560733795166\n",
      "188 6.22069787979126\n",
      "189 5.905076503753662\n",
      "190 5.605750560760498\n",
      "191 5.3216071128845215\n",
      "192 5.05222225189209\n",
      "193 4.796418190002441\n",
      "194 4.553955078125\n",
      "195 4.323831558227539\n",
      "196 4.105397701263428\n",
      "197 3.898218870162964\n",
      "198 3.701725721359253\n",
      "199 3.515061378479004\n",
      "200 3.3377816677093506\n",
      "201 3.1699161529541016\n",
      "202 3.0103933811187744\n",
      "203 2.8592448234558105\n",
      "204 2.715318441390991\n",
      "205 2.5788354873657227\n",
      "206 2.449592351913452\n",
      "207 2.326721668243408\n",
      "208 2.209998846054077\n",
      "209 2.0990841388702393\n",
      "210 1.993861436843872\n",
      "211 1.8939875364303589\n",
      "212 1.7993803024291992\n",
      "213 1.709411859512329\n",
      "214 1.6238309144973755\n",
      "215 1.5427380800247192\n",
      "216 1.465813398361206\n",
      "217 1.3925139904022217\n",
      "218 1.3229948282241821\n",
      "219 1.257049560546875\n",
      "220 1.194478154182434\n",
      "221 1.1349780559539795\n",
      "222 1.078507661819458\n",
      "223 1.0249511003494263\n",
      "224 0.973866879940033\n",
      "225 0.9253292679786682\n",
      "226 0.8793678879737854\n",
      "227 0.8357470035552979\n",
      "228 0.7942355275154114\n",
      "229 0.7548709511756897\n",
      "230 0.7173610329627991\n",
      "231 0.6818476319313049\n",
      "232 0.6480273008346558\n",
      "233 0.6159060001373291\n",
      "234 0.5854137539863586\n",
      "235 0.5564244389533997\n",
      "236 0.5289514660835266\n",
      "237 0.5028018951416016\n",
      "238 0.4779098629951477\n",
      "239 0.4542835056781769\n",
      "240 0.43193233013153076\n",
      "241 0.4105605483055115\n",
      "242 0.39042404294013977\n",
      "243 0.37111347913742065\n",
      "244 0.3527836799621582\n",
      "245 0.33542105555534363\n",
      "246 0.3189446032047272\n",
      "247 0.30317050218582153\n",
      "248 0.28826165199279785\n",
      "249 0.2740476727485657\n",
      "250 0.26059696078300476\n",
      "251 0.24777208268642426\n",
      "252 0.2355969399213791\n",
      "253 0.22403603792190552\n",
      "254 0.21305283904075623\n",
      "255 0.20256510376930237\n",
      "256 0.19264040887355804\n",
      "257 0.1831916868686676\n",
      "258 0.17419669032096863\n",
      "259 0.16565966606140137\n",
      "260 0.1575755625963211\n",
      "261 0.1498301923274994\n",
      "262 0.14249129593372345\n",
      "263 0.1355236917734146\n",
      "264 0.12890881299972534\n",
      "265 0.12262088060379028\n",
      "266 0.11661408841609955\n",
      "267 0.11092298477888107\n",
      "268 0.10550908744335175\n",
      "269 0.10035765916109085\n",
      "270 0.09545125812292099\n",
      "271 0.09081215411424637\n",
      "272 0.08637360483407974\n",
      "273 0.08217223733663559\n",
      "274 0.07815854251384735\n",
      "275 0.07433736324310303\n",
      "276 0.07071392238140106\n",
      "277 0.06727144867181778\n",
      "278 0.0639985203742981\n",
      "279 0.06089123710989952\n",
      "280 0.057918332517147064\n",
      "281 0.05511949211359024\n",
      "282 0.05245305597782135\n",
      "283 0.049894463270902634\n",
      "284 0.04748091101646423\n",
      "285 0.04518033191561699\n",
      "286 0.042980730533599854\n",
      "287 0.04090471565723419\n",
      "288 0.038921747356653214\n",
      "289 0.03705138713121414\n",
      "290 0.03525537997484207\n",
      "291 0.033560849726200104\n",
      "292 0.03192513436079025\n",
      "293 0.03037743829190731\n",
      "294 0.028919626027345657\n",
      "295 0.02750995382666588\n",
      "296 0.026201676577329636\n",
      "297 0.024946942925453186\n",
      "298 0.02374344877898693\n",
      "299 0.02261163853108883\n",
      "300 0.02153324894607067\n",
      "301 0.020489023998379707\n",
      "302 0.0195061806589365\n",
      "303 0.018568791449069977\n",
      "304 0.017680753022432327\n",
      "305 0.01683652028441429\n",
      "306 0.016028951853513718\n",
      "307 0.01526612788438797\n",
      "308 0.01454050000756979\n",
      "309 0.01385434065014124\n",
      "310 0.013196920044720173\n",
      "311 0.012567570433020592\n",
      "312 0.011975344270467758\n",
      "313 0.01140532921999693\n",
      "314 0.01086728647351265\n",
      "315 0.010358740575611591\n",
      "316 0.009864749386906624\n",
      "317 0.009407238103449345\n",
      "318 0.008966714143753052\n",
      "319 0.008546282537281513\n",
      "320 0.008154192008078098\n",
      "321 0.007768385577946901\n",
      "322 0.007414590567350388\n",
      "323 0.0070720394141972065\n",
      "324 0.006743552628904581\n",
      "325 0.006433544680476189\n",
      "326 0.0061424728482961655\n",
      "327 0.005865598563104868\n",
      "328 0.005593449808657169\n",
      "329 0.005341569427400827\n",
      "330 0.005103234201669693\n",
      "331 0.004874817561358213\n",
      "332 0.004656737670302391\n",
      "333 0.004451377782970667\n",
      "334 0.004249834455549717\n",
      "335 0.004063446074724197\n",
      "336 0.0038852274883538485\n",
      "337 0.003712247358635068\n",
      "338 0.00355396862141788\n",
      "339 0.0033987846691161394\n",
      "340 0.00325195980258286\n",
      "341 0.003111405298113823\n",
      "342 0.0029768990352749825\n",
      "343 0.0028513125143945217\n",
      "344 0.002732377965003252\n",
      "345 0.002615168457850814\n",
      "346 0.002506827935576439\n",
      "347 0.0024031512439250946\n",
      "348 0.002303021028637886\n",
      "349 0.002208077348768711\n",
      "350 0.0021185700315982103\n",
      "351 0.0020306811202317476\n",
      "352 0.0019507650285959244\n",
      "353 0.0018719024956226349\n",
      "354 0.001795999938622117\n",
      "355 0.001725642243400216\n",
      "356 0.001656839856877923\n",
      "357 0.0015943143516778946\n",
      "358 0.0015302415704354644\n",
      "359 0.0014723710482940078\n",
      "360 0.0014163247542455792\n",
      "361 0.001361632370389998\n",
      "362 0.001310796826146543\n",
      "363 0.001262109144590795\n",
      "364 0.0012120421743020415\n",
      "365 0.0011691524414345622\n",
      "366 0.001125857001170516\n",
      "367 0.0010856284061446786\n",
      "368 0.001046692836098373\n",
      "369 0.0010094596073031425\n",
      "370 0.0009731805184856057\n",
      "371 0.0009408196783624589\n",
      "372 0.0009083905024453998\n",
      "373 0.0008765501552261412\n",
      "374 0.00084542331751436\n",
      "375 0.0008156716357916594\n",
      "376 0.0007882365025579929\n",
      "377 0.0007612284971401095\n",
      "378 0.0007365324418060482\n",
      "379 0.0007125108386389911\n",
      "380 0.0006899508880451322\n",
      "381 0.0006679362850263715\n",
      "382 0.0006463497993536294\n",
      "383 0.000624774198513478\n",
      "384 0.0006049458170309663\n",
      "385 0.0005854518967680633\n",
      "386 0.000567177776247263\n",
      "387 0.0005504606524482369\n",
      "388 0.0005333948647603393\n",
      "389 0.0005162267480045557\n",
      "390 0.0005004907725378871\n",
      "391 0.00048449524911120534\n",
      "392 0.0004701587895397097\n",
      "393 0.0004567621508613229\n",
      "394 0.00044379872269928455\n",
      "395 0.00043003118480555713\n",
      "396 0.00041786342626437545\n",
      "397 0.00040640466613695025\n",
      "398 0.00039463298162445426\n",
      "399 0.0003838000993710011\n",
      "400 0.0003723705012816936\n",
      "401 0.00036213966086506844\n",
      "402 0.0003520752361509949\n",
      "403 0.0003425533068366349\n",
      "404 0.00033285855897702277\n",
      "405 0.00032404338708147407\n",
      "406 0.00031582958763465285\n",
      "407 0.0003075435815844685\n",
      "408 0.0003002774028573185\n",
      "409 0.00029175999225117266\n",
      "410 0.00028416747227311134\n",
      "411 0.0002765770477708429\n",
      "412 0.00027034495724365115\n",
      "413 0.000263307272689417\n",
      "414 0.00025628230650909245\n",
      "415 0.00025001855101436377\n",
      "416 0.00024470913922414184\n",
      "417 0.0002377432829234749\n",
      "418 0.00023200406576506793\n",
      "419 0.00022632759646512568\n",
      "420 0.00022049625113140792\n",
      "421 0.00021557690342888236\n",
      "422 0.00021107788779772818\n",
      "423 0.00020576492534019053\n",
      "424 0.0002006205468205735\n",
      "425 0.00019650912145152688\n",
      "426 0.0001916038163471967\n",
      "427 0.00018726891721598804\n",
      "428 0.0001828779058996588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429 0.0001787524961400777\n",
      "430 0.00017459774971939623\n",
      "431 0.0001703459129203111\n",
      "432 0.00016702547145541757\n",
      "433 0.0001630586921237409\n",
      "434 0.00015913098468445241\n",
      "435 0.00015652754518669099\n",
      "436 0.00015282558160834014\n",
      "437 0.00014949274191167206\n",
      "438 0.00014634917897637933\n",
      "439 0.00014341549831442535\n",
      "440 0.00014059308159630746\n",
      "441 0.00013798766303807497\n",
      "442 0.00013543413660954684\n",
      "443 0.00013244131696410477\n",
      "444 0.00012995539873372763\n",
      "445 0.00012726624845527112\n",
      "446 0.00012493676331359893\n",
      "447 0.00012244308891240507\n",
      "448 0.0001203226056532003\n",
      "449 0.00011782200454035774\n",
      "450 0.00011502904089866206\n",
      "451 0.00011307625391054899\n",
      "452 0.00011080759577453136\n",
      "453 0.00010920184286078438\n",
      "454 0.00010718530393205583\n",
      "455 0.00010493594891158864\n",
      "456 0.00010309080244041979\n",
      "457 0.00010094624303746969\n",
      "458 9.970759856514633e-05\n",
      "459 9.791643969947472e-05\n",
      "460 9.592665446689352e-05\n",
      "461 9.431871876586229e-05\n",
      "462 9.2730660981033e-05\n",
      "463 9.117438457906246e-05\n",
      "464 8.932536729844287e-05\n",
      "465 8.803009404800832e-05\n",
      "466 8.664794586366042e-05\n",
      "467 8.493725181324407e-05\n",
      "468 8.340997010236606e-05\n",
      "469 8.205600897781551e-05\n",
      "470 8.073875505942851e-05\n",
      "471 7.94528896221891e-05\n",
      "472 7.811854447936639e-05\n",
      "473 7.70004844525829e-05\n",
      "474 7.561962411273271e-05\n",
      "475 7.43721830076538e-05\n",
      "476 7.336692215176299e-05\n",
      "477 7.232322241179645e-05\n",
      "478 7.12820838089101e-05\n",
      "479 7.032545545371249e-05\n",
      "480 6.918473081896082e-05\n",
      "481 6.796949310228229e-05\n",
      "482 6.717732321703807e-05\n",
      "483 6.587019743165001e-05\n",
      "484 6.50512010906823e-05\n",
      "485 6.44815299892798e-05\n",
      "486 6.323034904198721e-05\n",
      "487 6.245381518965587e-05\n",
      "488 6.149942782940343e-05\n",
      "489 6.0527523601194844e-05\n",
      "490 5.9984369727317244e-05\n",
      "491 5.879096352146007e-05\n",
      "492 5.7793764426605776e-05\n",
      "493 5.712384881917387e-05\n",
      "494 5.604496254818514e-05\n",
      "495 5.5366919696098194e-05\n",
      "496 5.476655132952146e-05\n",
      "497 5.4145799367688596e-05\n",
      "498 5.334041634341702e-05\n",
      "499 5.252306436887011e-05\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "      # Forward pass: compute predicted y using operations on Tensors. Since w1 and\n",
    "      # w2 have requires_grad=True, operations involving these Tensors will cause\n",
    "      # PyTorch to build a computational graph, allowing automatic computation of\n",
    "      # gradients. Since we are no longer implementing the backward pass by hand we\n",
    "      # don't need to keep references to intermediate values.\n",
    "      y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "      # Compute and print loss. Loss is a Tensor of shape (), and loss.item()\n",
    "      # is a Python number giving its value.\n",
    "      loss = (y_pred - y).pow(2).sum()\n",
    "      print(t, loss.item())\n",
    "\n",
    "      # Use autograd to compute the backward pass. This call will compute the\n",
    "      # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "      # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "      # of the loss with respect to w1 and w2 respectively.\n",
    "      loss.backward()\n",
    "\n",
    "      # Update weights using gradient descent. For this step we just want to mutate\n",
    "      # the values of w1 and w2 in-place; we don't want to build up a computational\n",
    "      # graph for the update steps, so we use the torch.no_grad() context manager\n",
    "      # to prevent PyTorch from building a computational graph for the updates\n",
    "      with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after running the backward pass\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
